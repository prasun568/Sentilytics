# -*- coding: utf-8 -*-
"""social_media_sentiment_analyser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DQqtxJqNYtq8n-sbpN45Zl9eNETiaxJ4
"""

!pip install nltk scikit-learn pandas matplotlib seaborn

import pandas as pd
import numpy as np
import nltk
import re
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    text = re.sub(r"@\w+|#\w+", "", text)
    text = re.sub(r"[^a-z\s]", "", text)
    words = text.split()
    words = [w for w in words if w not in stop_words]
    return " ".join(words)

import pandas as pd

reddit_df = pd.read_csv("Reddit_Data.csv")
twitter_df = pd.read_csv("Twitter_Data.csv")
youtube_df = pd.read_csv("YoutubeCommentsDataSet.csv")
sentiment_df = pd.read_csv("sentimentdataset.csv")

print("Reddit columns:", reddit_df.columns)
print("Twitter columns:", twitter_df.columns)
print("YouTube columns:", youtube_df.columns)
print("Sentiment dataset columns:", sentiment_df.columns)

import pandas as pd

reddit_df = pd.read_csv("Reddit_Data.csv")
twitter_df = pd.read_csv("Twitter_Data.csv")
youtube_df = pd.read_csv("YoutubeCommentsDataSet.csv")
sentiment_df = pd.read_csv("sentimentdataset.csv")

reddit_df = reddit_df.rename(columns={
    "clean_comment": "text",
    "category": "sentiment"
})

twitter_df = twitter_df.rename(columns={
    "clean_text": "text",
    "category": "sentiment"
})

youtube_df = youtube_df.rename(columns={
    "Comment": "text",
    "Sentiment": "sentiment"
})

sentiment_df = sentiment_df.rename(columns={
    "Text": "text",
    "Sentiment": "sentiment"
})

def normalize_sentiment(val):
    if val == 2 or val == "2" or val == "positive" or val == "Positive":
        return "positive"
    elif val == 1 or val == "1" or val == "neutral" or val == "Neutral":
        return "neutral"
    else:
        return "negative"

reddit_df['sentiment'] = reddit_df['sentiment'].apply(normalize_sentiment)
twitter_df['sentiment'] = twitter_df['sentiment'].apply(normalize_sentiment)
youtube_df['sentiment'] = youtube_df['sentiment'].apply(normalize_sentiment)
sentiment_df['sentiment'] = sentiment_df['sentiment'].apply(normalize_sentiment)

reddit_df['source'] = 'Reddit'
twitter_df['source'] = 'Twitter'
youtube_df['source'] = 'YouTube'
sentiment_df['source'] = 'Mixed'

reddit_df = reddit_df[['text', 'sentiment', 'source']]
twitter_df = twitter_df[['text', 'sentiment', 'source']]
youtube_df = youtube_df[['text', 'sentiment', 'source']]
sentiment_df = sentiment_df[['text', 'sentiment', 'source']]

data = pd.concat(
    [reddit_df, twitter_df, youtube_df, sentiment_df],
    ignore_index=True
)

data.head()
data.shape

data = data.dropna(subset=['text', 'sentiment'])
data = data.drop_duplicates(subset=['text'])

data['clean_text'] = data['text'].apply(clean_text)

data['sentiment'].value_counts()

X = data['clean_text']     # input text
y = data['sentiment']      # labels

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(
    max_features=5000,   # keeps model efficient
    ngram_range=(1,2)    # unigrams + bigrams (better accuracy)
)

X_tfidf = tfidf.fit_transform(X)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_tfidf,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

from sklearn.naive_bayes import MultinomialNB

nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

y_pred = nb_model.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print("Naive Bayes Accuracy:", accuracy)

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred, labels=['negative','neutral','positive'])

sns.heatmap(cm, annot=True, fmt='d',
            xticklabels=['negative','neutral','positive'],
            yticklabels=['negative','neutral','positive'])

plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Naive Bayes")
plt.show()

def predict_sentiment_ml(text):
    text = clean_text(text)
    vec = tfidf.transform([text])
    return nb_model.predict(vec)[0]

predict_sentiment_ml("I really loved this video")
predict_sentiment_ml("This was a terrible experience")

!pip install transformers torch --quiet

from transformers import pipeline

bert_pipeline = pipeline(
    "sentiment-analysis",
    model="distilbert-base-uncased-finetuned-sst-2-english"
)

bert_pipeline("I really love this product!")

def bert_predict(text):
    # Truncate text to fit within BERT's max sequence length (512 tokens)
    truncated_text = text[:512]
    result = bert_pipeline(truncated_text)[0]
    return result['label'], result['score']

bert_predict("This is the worst experience ever")

bert_data = data.sample(1000, random_state=42)  # safe size

bert_data['bert_sentiment'] = bert_data['text'].apply(
    lambda x: bert_predict(x)[0]
)

comparison_df = bert_data[['text', 'sentiment', 'bert_sentiment']]
comparison_df.head()

bert_data['bert_sentiment'] = bert_data['bert_sentiment'].str.lower()

import matplotlib.pyplot as plt

bert_data['bert_sentiment'].value_counts().plot(kind='bar')
plt.title("BERT Sentiment Distribution")
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.show()

sample_text = "The video was good but the audio quality was bad"

print("ML Prediction:", predict_sentiment_ml(sample_text))
print("BERT Prediction:", bert_predict(sample_text))

# ===== INSTALLS =====
!pip install gradio transformers scikit-learn nltk pandas matplotlib --quiet

# ===== IMPORTS =====
import gradio as gr
import pandas as pd
import pickle, re, nltk
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from transformers import pipeline

# ===== NLTK =====
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# ===== LOAD MODEL & VECTORIZER =====
# Using the globally defined nb_model and tfidf objects directly
# as they are available in the kernel from previous cell executions.
model = nb_model
# tfidf is already globally defined and accessible, so no re-assignment is needed.

# ===== LOAD BERT =====
bert = pipeline(
    "sentiment-analysis",
    model="distilbert-base-uncased-finetuned-sst-2-english"
)

# ===== CLEAN TEXT =====
def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    text = re.sub(r"@\w+|#\w+", "", text)
    text = re.sub(r"[^a-z\s]", "", text)
    return " ".join([w for w in text.split() if w not in stop_words])

# ===== TEXT ANALYSIS =====
def analyze_text(text_input):
    if text_input.strip() == "":
        return "‚ùå Please enter some text"

    clean = clean_text(text_input)
    vec = tfidf.transform([clean])
    ml_pred = model.predict(vec)[0]

    bert_out = bert(text_input)[0]

    return (
        f"""
‚ú® **Sentiment Analysis Result**

üîπ **ML Sentiment:** {ml_pred}
üîπ **BERT Sentiment:** {bert_out['label']}
üîπ **Confidence Score:** {round(bert_out['score'],3)}
        """
    )

# ===== CSV ANALYSIS =====
def analyze_csv(csv_file, text_column):
    if csv_file is None:
        return "‚ùå Please upload a CSV file", None

    df = pd.read_csv(csv_file.name)

    if text_column not in df.columns:
        return f"‚ùå Column '{text_column}' not found in CSV", None

    df['clean_text'] = df[text_column].apply(clean_text)
    vec = tfidf.transform(df['clean_text'])
    df['sentiment'] = model.predict(vec)

    counts = df['sentiment'].value_counts()

    fig, ax = plt.subplots()
    ax.pie(
        counts.values,
        labels=counts.index,
        autopct='%1.1f%%',
        startangle=90
    )
    ax.set_title("üí¨ Sentiment Distribution (CSV Data)")

    summary = f"""
üìä **CSV Sentiment Summary**

üßæ Total Records: {len(df)}

üòä Positive: {counts.get('positive',0)}
üòê Neutral: {counts.get('neutral',0)}
üò° Negative: {counts.get('negative',0)}
    """

    return summary, fig


# ===== UI LAYOUT =====
with gr.Blocks(theme=gr.themes.Soft()) as demo:

    gr.Markdown("""
    #  Sentilytics
    ### Social Media Sentiment Analysis using ML & BERT

    Analyze public sentiment from **Text or CSV data**
    """)

    with gr.Tabs():

        # ----- TAB 1: TEXT -----
        with gr.Tab("üìù Analyze Text"):
            gr.Markdown("Enter a sentence to instantly detect sentiment")
            text_input = gr.Textbox(
                lines=4,
                placeholder="Type your text here‚Ä¶",
                label="Input Text"
            )
            text_btn = gr.Button("üîç Analyze Sentiment", variant="primary")
            text_output = gr.Markdown()

            text_btn.click(
                analyze_text,
                inputs=text_input,
                outputs=text_output
            )

        # ----- TAB 2: CSV -----
        with gr.Tab("üìÇ Analyze CSV"):
            gr.Markdown("üìä Upload bulk feedback data and visualize sentiment")
            csv_file = gr.File(label="Upload CSV file")
            text_column = gr.Textbox(
                value="text",
                label="Text Column Name",
                placeholder="e.g. text, review, comment"
            )
            csv_btn = gr.Button("üìà Analyze CSV", variant="primary")
            csv_text = gr.Markdown()
            csv_plot = gr.Plot()

            csv_btn.click(
                analyze_csv,
                inputs=[csv_file, text_column],
                outputs=[csv_text, csv_plot]
            )

    gr.Markdown("""
    ---
    üí° **Powered by:** TF-IDF + Naive Bayes & BERT
    üéì **Made by:** Prasun Singh & Praman Jain
    """)

demo.launch()

!git config --global user.name "prasun568"
!git config --global user.email "prasun976@gmail.com"

!git add .

!git commit -m "Initial commit: Social Media Sentiment Analyser"

!git remote remove origin
!git branch -M main
!git remote add origin https://prasun568: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@github.com/prasun568/social_media_sentiment_analyser.git
!git push -u origin main

used_packages = [
    'nltk',
    'scikit-learn',
    'pandas',
    'matplotlib',
    'seaborn',
    'numpy',
    'transformers',
    'torch',
    'gradio'
]

!pip freeze | grep -E "^({'|'.join(used_packages)})"

from google.colab import files
files.download("requirements.txt")

!git add requirements.txt
!git commit -m "Added requirements.txt"
!git pull origin main --rebase
!git push origin main

"""```
gradio==5.50.0
gradio_client==1.14.0
matplotlib==3.10.0
matplotlib-inline==0.2.1
matplotlib-venn==1.1.2
nltk==3.9.1
numpy==2.0.2
pandas==2.2.2
pandas-datareader==0.10.0
pandas-gbq==0.30.0
pandas-stubs==2.2.2.240909
scikit-learn==1.6.1
seaborn==0.13.2
torch==2.9.0+cpu
torchao==0.10.0
torchaudio==2.9.0+cpu
torchdata==0.11.0
torchsummary==1.5.1
torchtune==0.6.1
torchvision==0.24.0+cpu
transformers==4.57.3
```
"""

!ls -F

!git add . && git commit -m "Add all project files"

!git pull origin main --rebase
!git push origin main

!ls -F

!git add .
!git commit -m "Add notebook file to repository"
!git push origin main